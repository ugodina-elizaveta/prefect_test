import os
import json
import shutil
from io import BytesIO
from typing import List, Dict, Any

import fastavro
import pandas as pd
import psycopg2
from fastavro import writer, parse_schema
from kafka import KafkaProducer
from prefect import flow, task, get_run_logger
from psycopg2.extras import DictCursor

from shared.config import POSTGRES_CONFIG, KAFKA_CONFIG, AIRFLOW_DATA_PATH, ODATA_TASKS_GENERATOR_TABLE


@task(retries=1, retry_delay_seconds=300)
def fetch_data_and_save_to_files(
    directory_path: str,
    sql: str,
    avro_schema: dict,
    batch_size: int = 1000
) -> str:  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º directory_path –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ —Å–ª–µ–¥—É—é—â–∏–π —Ç–∞—Å–∫
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ PostgreSQL –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ —Ñ–∞–π–ª—ã Avro –±–∞—Ç—á–∞–º–∏.
    """
    logger = get_run_logger()
    logger.info(f"Starting data extraction to {directory_path}")

    # –û—á–∏—Å—Ç–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    if os.path.isdir(directory_path):
        if os.listdir(directory_path):
            raise FileExistsError(f"The directory '{directory_path}' already contains files.")
        shutil.rmtree(directory_path)
    os.makedirs(directory_path, exist_ok=True)

    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    conn = psycopg2.connect(**POSTGRES_CONFIG)
    cursor = conn.cursor(name='streaming_cursor', cursor_factory=DictCursor)
    cursor.itersize = batch_size

    # –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ö–µ–º—ã Avro
    parsed_schema = parse_schema(avro_schema)

    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL-–∑–∞–ø—Ä–æ—Å–∞
    cursor.execute(sql)

    batch_data = []
    file_counter = 1

    try:
        # –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        for row in cursor:
            batch_data.append(dict(row))

            # –ï—Å–ª–∏ –Ω–∞–∫–æ–ø–∏–ª—Å—è –±–∞—Ç—á, –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –µ–≥–æ –≤ —Ñ–∞–π–ª
            if len(batch_data) >= batch_size:
                file_path = os.path.join(directory_path, f'batch_{file_counter}.avro')
                with open(file_path, 'wb') as f:
                    writer(f, parsed_schema, batch_data)
                logger.info(f"Saved batch {file_counter} to {file_path}")

                # –û—á–∏—â–∞–µ–º –±–∞—Ç—á –∏ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —Ñ–∞–π–ª–æ–≤
                batch_data = []
                file_counter += 1

        # –ó–∞–ø–∏—Å—å –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö (–ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á)
        if batch_data:
            file_path = os.path.join(directory_path, f'batch_{file_counter}.avro')
            with open(file_path, 'wb') as f:
                writer(f, parsed_schema, batch_data)
            logger.info(f"Saved final batch {file_counter} to {file_path}")

    except Exception as e:
        logger.error(f"Error processing data: {e}")
        raise
    finally:
        cursor.close()
        conn.close()

    return directory_path


@task(retries=1, retry_delay_seconds=300)
def send_messages_to_kafka(directory_path: str, topic_name: str, task_format: str) -> None:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –∏—Ö –≤ Kafka –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.
    """
    logger = get_run_logger()
    logger.info(f"Starting Kafka publishing from {directory_path} to topic {topic_name}")

    if task_format not in ('avro', 'json'):
        raise ValueError(f"Unsupported format: {task_format}. Must be 'avro' or 'json'")

    # –°–æ–∑–¥–∞–µ–º Kafka producer
    producer = KafkaProducer(**KAFKA_CONFIG)

    try:
        for filename in os.listdir(directory_path):
            file_path = os.path.join(directory_path, filename)

            with open(file_path, 'rb') as f:
                file_data = f.read()

                if not file_data:
                    logger.warning(f"No data in {filename} for producing to Kafka")
                    continue

                if task_format == 'json':
                    try:
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Avro –≤ JSON
                        avro_file = BytesIO(file_data)
                        reader = fastavro.reader(avro_file)
                        records = [record for record in reader]
                        file_data = json.dumps(records).encode('utf-8')
                    except Exception as e:
                        logger.error(f"Failed to convert Avro to JSON for {filename}: {e}")
                        continue

                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Kafka
                producer.send(topic_name, value=file_data)
                logger.info(f"Sent {filename} to topic {topic_name}")

            # –£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –ø–æ—Å–ª–µ –æ—Ç–ø—Ä–∞–≤–∫–∏
            os.remove(file_path)
            logger.info(f"Sent and removed {filename}")

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã
        producer.flush()

    except Exception as e:
        logger.error(f"Error sending messages to Kafka: {e}")
        raise
    finally:
        producer.close()


@task(name="Get Parameters")
def get_params() -> List[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–∞—á –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.
    """
    logger = get_run_logger()

    conn = psycopg2.connect(**POSTGRES_CONFIG)

    sql = f"""
        SELECT id, "sql", avro_schema, directory_path, topic_name, batch_size, task_name, task_format
        FROM {ODATA_TASKS_GENERATOR_TABLE}
        WHERE is_enabled = true
        ORDER BY task_name;
    """

    try:
        df = pd.read_sql(sql, conn)
        tasks = df.to_dict(orient='records')

        for taska in tasks:
            taska['directory_path'] = AIRFLOW_DATA_PATH + taska['directory_path']
            # –ü–∞—Ä—Å–∏–º JSON —Å—Ç—Ä–æ–∫—É –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è avro_schema
            if isinstance(taska['avro_schema'], str):
                taska['avro_schema'] = json.loads(taska['avro_schema'])

        logger.info(f"Found {len(tasks)} enabled tasks")
        return tasks

    except Exception as e:
        logger.error(f"Error getting params: {e}")
        raise
    finally:
        conn.close()


@flow(
    name="Process Single Task",
    description="Processes a single data extraction and Kafka publishing task"
)
def process_single_task(param: Dict[str, Any]):
    """
    Subflow –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏.
    """
    logger = get_run_logger()
    task_name = param['task_name']
    
    logger.info(f"Processing task: {task_name}")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª—ã
    directory_path = fetch_data_and_save_to_files.with_options(
        name=f"üì• Extract Data - {task_name}"
    )(
        directory_path=param['directory_path'],
        sql=param['sql'],
        avro_schema=param['avro_schema'],
        batch_size=param['batch_size']
    )

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ Kafka (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ç–∞—Å–∫–∞)
    send_messages_to_kafka.with_options(
        name=f"üì§ Send to Kafka - {task_name}"
    )(
        directory_path=directory_path,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ç–∞—Å–∫–∞
        topic_name=param['topic_name'],
        task_format=param['task_format']
    )

    logger.info(f"Completed task: {task_name}")


@flow(
    name="ODATA Tasks Generator",
    description="Main flow for processing data extraction tasks and sending to Kafka",
    retries=1,
    retry_delay_seconds=300
)
def odata_tasks_flow():
    """
    –û—Å–Ω–æ–≤–Ω–æ–π flow –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Kafka.
    """
    logger = get_run_logger()

    try:
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Å–µ—Ö –∑–∞–¥–∞—á
        params_list = get_params()

        if not params_list:
            logger.info("No enabled tasks found")
            return

        # –°–æ–∑–¥–∞–µ–º subflow –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏
        for param in params_list:
            process_single_task.with_options(
                name=f"üîÑ Task Group: {param['task_name']}"
            )(param)

        logger.info("All tasks completed successfully")

    except Exception as e:
        logger.error(f"Flow failed: {e}")
        raise


if __name__ == "__main__":
    odata_tasks_flow()
